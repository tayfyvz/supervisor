import operator
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from pydantic import BaseModel
from typing import Annotated, Literal
from langchain_core.messages import SystemMessage, ToolMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, add_messages, END
from langgraph.prebuilt import ToolNode
from langchain_core.tools import tool, InjectedToolCallId
from langgraph.checkpoint.memory import MemorySaver
from datetime import datetime
from researcher import graph as research_agent
from copywriter import graph as copywriter_agent
from langgraph.types import Command, RunnableConfig

load_dotenv()

# Load the supervisor system prompt
supervisor_prompt = open("prompts/supervisor.md", "r").read()


class SupervisorState(BaseModel):
    """The state of the supervisor agent.

    The research_reports attribute is shared with the researcher agent. This allows us to share the research reports between the researcher and copywriter agents.
    """
    messages: Annotated[list, add_messages] = []
    research_reports: Annotated[list, operator.add] = []
    task_description: str | None = None


@tool
async def handoff_to_subagent(
        agent_name: Literal["researcher", "copywriter"],
        task_description: str,
        tool_call_id: Annotated[str, InjectedToolCallId],
):
    """Assign a task to a sub-agent: researcher or copywriter.

    Args:
        agent_name: The name of the agent to handoff the task to. Valid agent names are researcher and copywriter.
        task_description: The description of the task to be completed.
    """
    # Construct the update schema for the Command primitive
    # We're specifying to update the task description in the state and add a tool message to the conversation to let the supervisor know the task has been handed off.
    update = {
        "task_description": task_description,
        "messages": [ToolMessage(
            name=f"handoff_to_{agent_name}",
            content=f"Successfully handed off task to {agent_name}.",
            tool_call_id=tool_call_id,
        )],
    }

    # Return the Command primitive with the update to the state and specifying the next node to go to
    return Command(
        goto=f"call_{agent_name}",
        update=update
    )


async def call_researcher(state: SupervisorState, config: RunnableConfig):
    """Call the researcher agent.

    The agent is invoked only with the task description generated by the supervisor, so the context window is not cluttered with the full conversation history of the supervisor.
    """
    research_response = await research_agent.ainvoke(
        input={
            "messages": [HumanMessage(content=state.task_description)],
        },
        config=config,
    )

    ai_message = AIMessage(name="researcher", content=research_response["messages"][-1].content)

    return {
        "research_reports": research_response["research_reports"],
        "messages": [ai_message],
    }


async def call_copywriter(state: SupervisorState, config: RunnableConfig):
    """Call the copywriter agent.

    The agent is invoked only with the task description generated by the supervisor, and any research reports that have been generated by the researcher.
    """
    copywriter_response = await copywriter_agent.ainvoke(
        input={
            "messages": [HumanMessage(content=state.task_description)],
            "research_reports": state.research_reports,
        },
        config=config,
    )

    ai_message = AIMessage(name="copywriter", content=copywriter_response["messages"][-1].content)

    return {"messages": [ai_message]}


llm = ChatOpenAI(
    name="Supervisor",
    model="gpt-5-mini-2025-08-07",
    reasoning_effort="low",
)

tools = [handoff_to_subagent]
llm_with_tools = llm.bind_tools(tools, parallel_tool_calls=False)


async def supervisor(state: SupervisorState):
    """The main supervisor agent."""
    response = llm_with_tools.invoke([
                                         SystemMessage(
                                             content=supervisor_prompt.format(current_datetime=datetime.now()))
                                     ] + state.messages)
    return {"messages": [response]}


async def supervisor_router(state: SupervisorState) -> str:
    """Route to the tools node if the supervisor makes a tool call."""
    if state.messages[-1].tool_calls:
        return "tools"
    return END


builder = StateGraph(SupervisorState)

builder.add_node(
    supervisor,
    # This is a new feature and is only needed for visualization, doesn't impact runtime
    # destinations=("tools", "call_researcher", "call_copywriter", END)
)
builder.add_node("tools", ToolNode(tools))
builder.add_node(call_researcher)
builder.add_node(call_copywriter)

builder.set_entry_point("supervisor")

builder.add_conditional_edges(
    "supervisor",
    supervisor_router,
    {
        "tools": "tools",
        END: END,
    }
)
# Normally we route back to the agent after a tool call. In this case we're using the Command primitive in the tool call to do the routing. So we don't need this typical edge.
# builder.add_edge("tools", "supervisor")

# Now every time we call a sub-agent we need to route back to the supervisor
builder.add_edge("call_researcher", "supervisor")
builder.add_edge("call_copywriter", "supervisor")

graph = builder.compile(checkpointer=MemorySaver())

# Visualize the graph
from IPython.display import Image

Image(graph.get_graph(xray=True).draw_mermaid_png())