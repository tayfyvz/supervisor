import operator
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from pydantic import BaseModel
from typing import Annotated, List
from langchain_core.messages import SystemMessage, ToolMessage
from langgraph.graph import StateGraph, add_messages, END
from langgraph.prebuilt import ToolNode
from langchain_core.tools import tool, InjectedToolCallId
from langgraph.checkpoint.memory import MemorySaver
from langchain_tavily import TavilySearch, TavilyExtract
from datetime import datetime
from langgraph.types import Command

load_dotenv()

# Load the researcher system prompt
researcher_prompt = open("prompts/researcher.md", "r").read()


@tool
async def search_web(
        query: str,
        num_results: int = 3
):
    """Search the web and get back a list of search results including the page title, url, and a short summary of each webpage.

    Args:
        query: The search query.
        num_results: The number of results to return, max is 3.

    Returns:
        A dictionary of the search results.
    """
    web_search = TavilySearch(max_results=min(num_results, 3), topic="general")
    search_results = web_search.invoke(input={"query": query})

    processed_results = {
        "query": query,
        "results": []
    }

    # Light processing of the search results to return a subset of the data
    for result in search_results["results"]:
        processed_results["results"].append({
            "title": result["title"],
            "url": result["url"],
            "content_preview": result["content"]
        })

    return processed_results


@tool
async def extract_content_from_webpage(urls: List[str]):
    """Extract the content from a webpage.

    Args:
        url: The url of the webpage to extract content from.

    Returns:
        A list of dictionaries containing the extracted content from each webpage.
    """
    web_extract = TavilyExtract()
    results = web_extract.invoke(input={"urls": urls})["results"]
    return results


class ResearchReport(BaseModel):
    topic: str
    report: str


@tool
async def generate_research_report(
        topic: str,
        report: str,
        tool_call_id: Annotated[str, InjectedToolCallId],
):
    """Generate a research report on a specific topic.

    Args:
        topic: The topic to research.
        report: The research report.

    Returns:
        The research report.
    """
    research_report = ResearchReport.model_validate({
        "topic": topic,
        "report": report
    })

    # We use the Command primitive to update the state with the research report and add a tool message to the conversation with the generated report.
    return Command(update={
        "research_reports": [research_report],
        "messages": [ToolMessage(
            name="generate_research_report",
            content=research_report.model_dump_json(),
            tool_call_id=tool_call_id,
        )],
    })


class ResearcherState(BaseModel):
    """The state of the researcher agent.

    The research_reports attribute is shared with the supervisor state. This allows the supervisor to access the research reports generated by the researcher and share them with the copywriter.
    """
    messages: Annotated[list, add_messages] = []
    research_reports: Annotated[list, operator.add] = []


tools = [
    search_web,
    extract_content_from_webpage,
    generate_research_report,
]

llm = ChatOpenAI(
    name="Researcher",
    model="gpt-5-mini-2025-08-07",
    reasoning_effort="minimal",
)
llm_with_tools = llm.bind_tools(tools)


async def researcher(state: ResearcherState):
    """The main researcher agent."""
    response = llm_with_tools.invoke([
                                         SystemMessage(
                                             content=researcher_prompt.format(current_datetime=datetime.now()))
                                     ] + state.messages)
    return {"messages": [response]}


async def researcher_router(state: ResearcherState) -> str:
    """Route to the tools node if the researcher makes a tool call."""
    if state.messages[-1].tool_calls:
        return "tools"
    return END


builder = StateGraph(ResearcherState)

builder.add_node(researcher)
builder.add_node("tools", ToolNode(tools))

builder.set_entry_point("researcher")
builder.add_edge("tools", "researcher")
builder.add_conditional_edges(
    "researcher",
    researcher_router,
    {
        "tools": "tools",
        END: END,
    }
)

# Don't use a checkpointer if using as a subgraph, the parent graph's checkpointer will be used
graph = builder.compile()

# graph = builder.compile(checkpointer=MemorySaver())

# Visualize the graph
# from IPython.display import Image
# Image(graph.get_graph().draw_mermaid_png())